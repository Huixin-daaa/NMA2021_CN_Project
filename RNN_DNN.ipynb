{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RNN_DNN.ipynb","provenance":[],"authorship_tag":"ABX9TyO79d1J12A6tmVtazEj0JNI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"TH4-fDrLPt66"},"source":["#reservoir\n","#Trial 1: Deep Neural Network\n","class DeepNetReLU(nn.Module):\n","  \"\"\" network with a single hidden layer h with a RELU \"\"\"\n","\n","  def __init__(self, n_inputs, n_hidden):\n","    super().__init__()  # needed to invoke the properties of the parent class nn.Module\n","    self.in_layer = nn.Linear(n_inputs, n_hidden) # neural activity --> hidden units\n","    self.out_layer = nn.Linear(n_hidden, 1) # hidden units --> output\n","\n","  def forward(self, r):\n","\n","    ############################################################################\n","    ## TO DO for students: write code for computing network output using a\n","    ## rectified linear activation function for the hidden units\n","    # Fill out function and remove\n","    #raise NotImplementedError(\"Student exercise: complete DeepNetReLU forward\")\n","    ############################################################################\n","\n","    h = torch.relu(self.in_layer(r)) # h is size (n_inputs, n_hidden)\n","    y = self.out_layer(h) # y is size (n_inputs, 1)\n","\n","\n","    return y\n","\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FE5Fb9ZxPyes"},"source":["def train(net, loss_fn, train_data, train_labels,\n","          n_epochs=50, learning_rate=1e-4):\n","  \"\"\"Run gradient descent to opimize parameters of a given network\n","\n","  Args:\n","    net (nn.Module): PyTorch network whose parameters to optimize\n","    loss_fn: built-in PyTorch loss function to minimize\n","    train_data (torch.Tensor): n_train x n_neurons tensor with neural\n","      responses to train on\n","    train_labels (torch.Tensor): n_train x 1 tensor with orientations of the\n","      stimuli corresponding to each row of train_data\n","    n_epochs (int, optional): number of epochs of gradient descent to run\n","    learning_rate (float, optional): learning rate to use for gradient descent\n","\n","  Returns:\n","    (list): training loss over iterations\n","\n","  \"\"\"\n","\n","  # Initialize PyTorch SGD optimizer\n","  optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n","\n","  # Placeholder to save the loss at each iteration\n","  train_loss = []\n","\n","  # Loop over epochs\n","  for i in range(n_epochs):\n","\n","    ######################################################################\n","    ## TO DO for students: fill in missing code for GD iteration\n","    #raise NotImplementedError(\"Student exercise: write code for GD iterations\")\n","    ######################################################################\n","\n","     # compute network output from inputs in train_data\n","    out = net(train_data)  # compute network output from inputs in train_data\n","\n","    # evaluate loss function\n","    loss = loss_fn(out, train_labels)\n","\n","    # Clear previous gradients\n","    optimizer.zero_grad()\n","\n","    # Compute gradients\n","    loss.backward()\n","\n","    # Update weights\n","    optimizer.step()\n","\n","\n","\n","    # Store current value of loss\n","    train_loss.append(loss.item())  # .item() needed to transform the tensor output of loss_fn to a scalar\n","\n","    # Track progress\n","    if (i + 1) % (n_epochs // 5) == 0:\n","      print(f'iteration {i + 1}/{n_epochs} | loss: {loss.item():.3f}')\n","\n","  return train_loss\n","\n","\n","# Set random seeds for reproducibility\n","np.random.seed(1)\n","torch.manual_seed(1)\n","\n","# Initialize network with 10 hidden units\n","net = DeepNetReLU(model['N'], 10)\n","\n","# Initialize built-in PyTorch MSE loss function\n","loss_fn = nn.MSELoss()\n","\n","#firing_rates_tensor = torch.from_numpy(firing_rates)\n","\n","# Run gradient descent on data\n","#(?)train_loss = train(net, loss_fn, firing_rates, input_stream) #resp_train, stimuli_train\n","\n","# Plot the training loss over iterations of GD\n","#(?)plot_train_loss(train_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NlUSxmzh9ff-"},"source":["#for trial 1\n","#Plotting Functions\n","def plot_train_loss(train_loss):\n","  plt.plot(train_loss)\n","  plt.xlim([0, None])\n","  plt.ylim([0, None])\n","  plt.xlabel('iterations of gradient descent')\n","  plt.ylabel('mean squared error')\n","  plt.show()"],"execution_count":null,"outputs":[]}]}